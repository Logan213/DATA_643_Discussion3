{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 643: Discussion 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROBLEM STATEMENT\n",
    "\n",
    "As more systems and sectors are driven by predictive analytics, there is increasing awareness of the possibility and pitfalls of algorithmic discrimination. In what ways do you think Recommender Systems reinforce human bias? Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### RESPONSE  \n",
    "\n",
    "As more systems and processes are driven by predictive analytics, discrimination by algorithm may become more commonplace. There have already been several stories in recent years about groups of people being offered or presented with a certain product (such as higher rate loans based on race) as the result of the algorithm and the data used in the system.  \n",
    "\n",
    "Recommender systems and their algorithms can be written and tuned to either reinforce or prevent unethical targeting. In some cases, the unethical targeting is intentional, much in the same way predatory lending can occur without the assistance of machines. And in others, the targeting or segmentation was accidental, as there are trends and groupings that occur naturally in society. Data scientists and programmers who design and deploy recommender systems will always need to check for accuracy and tune the system, just like anything else.  \n",
    "\n",
    "One interesting type of human bias is that which occurs from viewing the aggregated or overall rating for a product, item, movie etc. Studies have shown that humans will be more inclined to rate something higher (or lower), if the item already has a high or low rating. In my own unscientific observations, I generally tend to see most ratings (Yelp, Amazon, etc.) averaging as a \"very good\" rating (4/5 or 3.5/4 stars, etc.), unless there was anything wrong, then it's an automatic \"worst\" rating (0 or 1 star).  \n",
    "\n",
    "The popularity of an item also sometimes introduces bias. For instance, if a user is rating their three favorite composers of orchestral music, they may say Beethoven, Mozart, and John Williams. There's nothing wrong with any of them, but someone more interested in classical music may argue that John Williams (being a composer of modern film scores) has nothing in common with the other two. \n",
    "\n",
    "Regarding the first point, I found an interesting paper on rating bias in recommender systems from UC Berkeley. In short, the training set utilizes ratings before the users knew the median rating, after finding out the median, and the median rating itself. The objective is to mitigate any crowd bias or \"herd mentality\" that can undermine ratings, and therefore, recommendations of items to users. Some of the solutions to this were to either simply use the training set, or, try to predict whether a user would change their rating of a particular item. The end results of the experiment did not yield a lot of accuracy, but other methods such as clustering similar items were mentioned to try and improve accuracy. Regardless of the ability to predict bias, I think the paper makes a good point on something to consider when creating a recommendation system for any type of content.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESOURCES:  \n",
    "\n",
    "- http://goldberg.berkeley.edu/pubs/sanjay-recsys-v10.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
